import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline
import joblib
import shap

# 1. Cargar datos y exploración inicial
def load_data(csv_path):
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Archivo no encontrado: {csv_path}")
    data = pd.read_csv(csv_path)
    print("Datos cargados con éxito:\n", data.head())
    print("\nResumen de los datos:\n", data.describe())
    print("\nValores nulos en los datos:\n", data.isnull().sum())
    return data

# 2. Agrupar tipos de Pokémon en categorías generales
def group_pokemon_types(data):
    categories = {
        "Elementales": ["Agua", "Fuego", "Eléctrico", "Hielo", "Planta", "Tierra", "Roca"],
        "Místicos": ["Psíquico", "Dragón", "Fantasma", "Siniestro", "Hada", "Acero"],
        "Animales": ["Normal", "Lucha", "Volador", "Veneno", "Insecto"],
    }
    translation_dict = {
        "Water": "Agua", "Fire": "Fuego", "Electric": "Eléctrico", "Ice": "Hielo",
        "Grass": "Planta", "Ground": "Tierra", "Poison": "Veneno", "Steel": "Acero",
        "Rock": "Roca", "Dragon": "Dragón", "Ghost": "Fantasma", "Normal": "Normal",
        "Fighting": "Lucha", "Flying": "Volador", "Psychic": "Psíquico", "Bug": "Insecto",
        "Dark": "Siniestro", "Fairy": "Hada"
    }

    def map_types(type_str):
        if not type_str: return "Desconocido"
        translated = [translation_dict.get(t.strip(), t.strip()) for t in type_str.split(", ")]
        for category, types in categories.items():
            if any(t in translated for t in types):
                return category
        return "Desconocido"

    data['Grouped_Type'] = data['Type'].apply(map_types)
    print("\nDistribución de Tipos Agrupados:\n", data['Grouped_Type'].value_counts())
    return data

# 3. Preprocesar datos
def preprocess_data(data):
    X = data[['HP Base', 'Attack Base', 'Defense Base', 'Speed Base', 'Special Attack Base', 'Special Defense Base']]
    y = data['Grouped_Type']

    # Filtrar clases con suficientes ejemplos
    valid_classes = y.value_counts()[y.value_counts() >= 5].index
    data = data[data['Grouped_Type'].isin(valid_classes)]
    y, classes = pd.factorize(data['Grouped_Type'])

    # Escalar y balancear datos
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

    # Dividir datos
    X_train, X_test, y_train, y_test = train_test_split(
        X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42
    )

    return X_train, X_test, y_train, y_test, classes, scaler

# 4. Evaluar teorías de decisión
def analyze_decision_thresholds(model, X_test, y_test, classes):
    y_proba = model.predict_proba(X_test)
    for i, class_name in enumerate(classes):
        fpr, tpr, thresholds = roc_curve(y_test == i, y_proba[:, i])
        optimal_idx = np.argmax(tpr - fpr)
        optimal_threshold = thresholds[optimal_idx]

        plt.plot(fpr, tpr, label=f'{class_name} (AUC = {roc_auc_score(y_test == i, y_proba[:, i]):.2f})')
        print(f"Clase {class_name}: Umbral óptimo de decisión = {optimal_threshold:.2f}")

    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.xlabel('FPR')
    plt.ylabel('TPR')
    plt.title('Curvas ROC')
    plt.legend()
    plt.show()

# 5. Entrenar y evaluar modelos
def train_and_evaluate_models(X_train, X_test, y_train, y_test, classes):
    models = {
        "Random Forest": RandomForestClassifier(random_state=42),
        "SVM": SVC(probability=True),
        "XGBoost": XGBClassifier(eval_metric='mlogloss'),
        "KNN": KNeighborsClassifier(),
        "Naive Bayes": GaussianNB(),
        "Árbol de Decisión": DecisionTreeClassifier()
    }

    results = {}
    best_model = None
    best_score = 0

    for name, model in models.items():
        print(f"\nEntrenando modelo: {name}...")
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        acc = np.mean(y_test == y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        results[name] = {"Accuracy": acc, "F1-Score": f1}

        print(classification_report(y_test, y_pred, target_names=classes))
        sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap="Blues", fmt="d", xticklabels=classes, yticklabels=classes)
        plt.title(f"Matriz de Confusión: {name}")
        plt.show()

        # Mostrar ejemplos concretos de predicciones
        print("\n--- Ejemplos de predicciones ---")
        test_examples = pd.DataFrame(X_test[:5], columns=['HP Base', 'Attack Base', 'Defense Base', 'Speed Base', 'Special Attack Base', 'Special Defense Base'])
        test_examples['Predicción'] = [classes[label] for label in y_pred[:5]]
        test_examples['Etiqueta Real'] = [classes[label] for label in y_test[:5]]
        print(test_examples)

        if f1 > best_score:
            best_model = model
            best_score = f1

    # Interpretabilidad Global: Feature Importance
    if hasattr(best_model, 'feature_importances_'):
        feature_importance = best_model.feature_importances_
        plt.barh(range(len(feature_importance)), feature_importance, tick_label=[
            'HP Base', 'Attack Base', 'Defense Base', 'Speed Base', 'Special Attack Base', 'Special Defense Base'
        ])
        plt.title('Importancia de características (Mejor Modelo)')
        plt.show()

    # Análisis de Teoría de la Decisión
    if hasattr(best_model, 'predict_proba'):
        analyze_decision_thresholds(best_model, X_test, y_test, classes)

    # Análisis con SHAP
    shap_analysis(best_model, X_train, X_test, classes)

    return results, best_model


# Interpretabilidad Local y Global con SHAP
def shap_analysis(best_model, X_train, X_test, classes):
    if hasattr(best_model, 'predict_proba'):  # SHAP soporta modelos con predict_proba
        explainer = shap.Explainer(best_model, X_train)
        shap_values = explainer(X_test, check_additivity=False)

        # Interpretabilidad Local
        print("\nInterpretabilidad Local (ejemplo individual):")
        example_index = 0  # Cambia este índice para analizar otras instancias
        for i, class_name in enumerate(classes):
            print(f"Clase: {class_name}")
            single_shap_values = shap.Explanation(
                values=shap_values.values[example_index, :, i],
                base_values=shap_values.base_values[example_index, i],
                data=X_test[example_index],
                feature_names=['HP Base', 'Attack Base', 'Defense Base', 'Speed Base', 'Special Attack Base', 'Special Defense Base']
            )
            shap.plots.waterfall(single_shap_values)

             # Validar si es multiclase
        if len(shap_values.values.shape) == 3:  # Multiclase
            print("Análisis multiclase detectado. Promediando valores SHAP por clase.")
            shap_values_combined = np.mean(shap_values.values, axis=2)  # Promedio de las clases
        else:  # Binario o clase única
            shap_values_combined = shap_values.values

        print("\nDimensiones ajustadas de shap_values_combined:", shap_values_combined.shape)
        print("Dimensiones de X_test:", X_test.shape)

        # Interpretabilidad Global
        print("\nInterpretabilidad Global (importancia promedio):")
        shap.summary_plot(
            shap_values_combined,
            X_test,
            feature_names=['HP Base', 'Attack Base', 'Defense Base', 'Speed Base', 'Special Attack Base', 'Special Defense Base']
        )

# 6. Flujo principal del programa
def main():
    csv_path = 'pokemonDB_dataset.csv'

    data = load_data(csv_path)
    data = group_pokemon_types(data)

    X_train, X_test, y_train, y_test, classes, scaler = preprocess_data(data)

    results, best_model = train_and_evaluate_models(X_train, X_test, y_train, y_test, classes)

    print("\nResultados finales de los modelos:")
    for model_name, metrics in results.items():
        print(f"{model_name}: Accuracy = {metrics['Accuracy']:.2f}, F1-Score = {metrics['F1-Score']:.2f}")

    # Guardar el mejor modelo y el escalador
    model_path = 'best_model.joblib'
    scaler_path = 'scaler.joblib'
    joblib.dump(best_model, model_path)
    joblib.dump(scaler, scaler_path)
    print(f"Mejor modelo guardado en: {model_path}")
    print(f"Escalador guardado en: {scaler_path}")

# Ejecutar el flujo principal
if __name__ == "__main__":
    main()